# Enter your network definition here.
# Use Shift+Enter to update the visualization.
# Enter your network definition here.
# Use Shift+Enter to update the visualization.name:"alabo_chepai_attention_simple"
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "recog_label1"
  include { phase: TRAIN }
  transform_param {
    #scale: 0.00390625
	scale: 0.0078125
	mean_value: 128
  }
	image_data_param{
	   new_height: 512
	   new_width: 192
	   source: "/data/liuxu/container_qan/caffe_file_list_train.txt" #unchage
	   root_folder: "" #unchage
	   num_labels:20
	   batch_size:16 #reshape_x has batch_size
	   shuffle: false
	}
}


layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "recog_label1"
  include { phase: TEST }
  transform_param {
    #scale: 0.00390625
	scale: 0.0078125
	mean_value: 128
  }
  image_data_param{
    new_height: 512
    new_width: 192
    source: "/data/liuxu/container_qan/caffe_file_list_train.txt" #unchage
    root_folder: "" #unchage
    num_labels: 20
    batch_size: 16
    shuffle: false
  }
}

#layer {
#  name: "data"
#  type: "Input"
#  top: "data"
#  input_param { shape: { dim: 32 dim: 3 dim: 512 dim: 192 } }
#}


layer {
  name: "split_pic"
  type: "Slice"
  bottom: "data"
  top: "data1"
  top: "data2"
  top: "data3"
  top: "data4"
  top: "data5"
  top: "data6"
  top: "data7"
  top: "data8"
  slice_param {
    axis: 2
    slice_point: 64
    slice_point: 128
    slice_point: 192
    slice_point: 256
    slice_point: 320
    slice_point: 384
    slice_point: 448
  }
}

layer {
  name: "concat_pic"
  type: "Concat"
  bottom: "data1"
  bottom: "data2"
  bottom: "data3"
  bottom: "data4"
  bottom: "data5"
  bottom: "data6"
  bottom: "data7"
  bottom: "data8"
  top: "pic_data"
  concat_param {
    axis: 0
  }
}

layer {
  name: "permutelabel"
  type: "Permute"
  bottom: "recog_label1"
  top: "recog_label"
  permute_param {
  order: 1
  order: 0
  }
}
#####################################################################################

# delate stn
######################################################################################
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "pic_data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
	pad: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

 layer {  
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"  
  top: "relu1"  
}  

layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2 # pool over a 2x2 region
    stride: 2      # step two pixels (in the bottom blob) between pooling regions
  }
}
######################################################################################
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
	pad: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

 layer {  
  name: "relu2"  
  type: "ReLU"  
  bottom: "conv2"  
  top: "relu2"  
}

layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2 # pool over a 2x2 region
    stride: 2      # step two pixels (in the bottom blob) between pooling regions
  }
}
######################################################################################

layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 3
    stride: 1
	pad: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "conv3_bn"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3_bn"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
}

 layer {  
  name: "relu3"  
  type: "ReLU"  
  bottom: "conv3_bn"  
  top: "relu3"
}
######################################################################################
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 3
    stride: 1
	pad: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

 layer {
  name: "relu4"  
  type: "ReLU"  
  bottom: "conv4"  
  top: "relu4"
}

layer {
  name: "pool3"
  type: "Pooling"
  bottom: "relu4"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2 # pool over a 2x2 region
    stride_w: 1
	stride_h: 2
	pad_w: 1
	pad_h: 0
  }
}
######################################################################################
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "pool3"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 3
    stride: 1
	pad: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "conv5_bn"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5_bn"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
}

 layer {
  name: "relu5"  
  type: "ReLU"  
  bottom: "conv5_bn"  
  top: "relu5"
}
######################################################################################
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "relu5"
  top: "conv6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 3
    stride: 1
	pad: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

 layer {
  name: "relu6"  
  type: "ReLU"  
  bottom: "conv6"  
  top: "relu6"
}

layer {
  name: "pool4"
  type: "Pooling"
  bottom: "relu6"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2 # pool over a 2x2 region
    stride_w: 1
	stride_h: 2
	pad_w: 1
	pad_h: 0
  }
}
######################################################################################
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "pool4"
  top: "conv7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 1
	pad: 0
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "conv7_bn"
  type: "BatchNorm"
  bottom: "conv7"
  top: "conv7_bn"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
}

 layer {
  name: "relu7"  
  type: "ReLU"  
  bottom: "conv7_bn"  
  top: "relu7"
}

############################################################################
# generate score
layer {
  name: "fc1_s"
  type: "InnerProduct"
  bottom: "pool4"
  top: "fc1_s"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
	#weight_filler {
     # type: "xavier"
    #}
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer{
    name: "reshape_s1"
    bottom:"fc1_s"
    top:"reshape_s1"
    type:"Reshape"
    reshape_param{
        shape{
            dim:8
            dim:-1
        }
    }
}

layer{
	name: "softmax_s1"
	bottom: "reshape_s1"
	top: "softmax_s1"
	type: "Softmax"
    softmax_param{
        axis: 0
    }
}

layer{
    name: "reshape_s2"
    bottom:"softmax_s1"
    top:"reshape_s2"
    type:"Reshape"
    reshape_param{
        shape{
            dim:-1
            dim:1
        }
    }
}

layer{
    name:"tile_s1"
    bottom:"reshape_s2"
    top:"tile_s1"
    type:"Tile"
    tile_param{
      axis:1
      tiles:37632 #before 7936
    }
}

layer{
    name:"reshape_s3"
    bottom:"tile_s1"
    top:"reshape_s3"
    type:"Reshape"
    reshape_param{
        shape{
            dim:-1
            dim:256
            dim:3 #1
            dim:49 #before 31 
        }
    }
}

# now[256,256,3,49]

#end score generation
########################################################################
#fusion feature
layer{
	name: "elt_feature"
	bottom: "relu7"
	bottom: "reshape_s3"
	top: "elt_feature"
	type: "Eltwise"
    eltwise_param{
        operation: PROD
    }
}

layer{
    name:"reshape_s4"
    bottom:"elt_feature"
    top:"reshape_s4"
    type:"Reshape"
    reshape_param{
        shape{
            dim:8
            dim:-1
            dim:256
            dim:49 #31
        }
    }
}

layer{
    name:"slice_seq"
    bottom:"reshape_s4"
    top:"seq1"
    top:"seq2"
    top:"seq3"
    top:"seq4"
    top:"seq5"
    top:"seq6"
    top:"seq7"
    top:"seq8"
    type:"Slice"
    slice_param{
        axis: 0
        slice_point: 1
        slice_point: 2
        slice_point: 3
        slice_point: 4
        slice_point: 5
        slice_point: 6
        slice_point: 7
    }
}

layer{
    name:"seq_sum"
    bottom:"seq1"
    bottom:"seq2"
    bottom:"seq3"
    bottom:"seq4"
    bottom:"seq5"
    bottom:"seq6"
    bottom:"seq7"
    bottom:"seq8"
    top:"seq_sum"
    type:"Eltwise"
    eltwise_param{
        operation:SUM
    }
}
# now[1,96,256,49]
#this should be change dimensions to (batch_size?,256,1,h*w)[32,256,1,49*3=147]

layer{
    name:"seq_reshape"
    bottom:"seq_sum"
    top:"seq_sum_reshape"
    type:"Reshape"
    reshape_param{
        shape{
            dim: -1
            dim: 256
            dim: 1
            dim: 147 # 31
        }
    }
}

#end feature fusion

############################################



##########################################

# compute cont what's this
layer {
  name: "cont"
  type: "SeqMask"
  bottom: "recog_label"
  top: "cont"
  att_param {
	eos: 38 # what's this ours 38
  }
}

  
# shift one label
layer {
  name: "shiftseq1"
  type: "ShiftSeq"
  bottom: "recog_label"
  top: "recog_label_input"
  att_param{
    bos: 36 # what's this ours 36
  }
}

######################################################################################

layer {
# 65 * 512
  name: "axis-swap"
  type: "AxisSwap"
  bottom: "seq_sum_reshape"
  top: "concat_pool"
  axis_swap_param { 
    max_width: 147 #31 
  }
}


# ours
layer {
    name: "reshape_x"
    type: "Reshape"
    bottom: "concat_pool"
    top: "concat_pool1"
    reshape_param {
      shape {
        dim: -1  # inputLength need change
        dim: 16  # batch size
        dim: 256
        #dim: -1 # infer it from the other dimensions
      }
    }
}

  
# 100 * 2 * 512
layer {
  name: "RNNDecWT"
  type: "RNNDecWT"
  bottom: "concat_pool1"
  bottom: "cont"
  bottom: "recog_label_input"
  top: "RNNDec"
  recurrent_param {
    num_output: 256
    weight_filler {
      type: "uniform"
      min: -0.08
      max: 0.08
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }

  att_param {
    type: 0
#	loc_dim: 256
#	loc_size: 5
	bos: 36
	embed_input_dim: 39
	embed_num_output: 100
  }
}

######################################################################################
layer {
  name: "SoftmaxLoss"
  type: "AttSoftmaxWithLoss"
  bottom: "RNNDec"
  bottom: "recog_label"
  top: "SoftmaxLoss"
  loss_weight: 1
  ctc_param {
    axis: 2
  }
  softmax_param {
    axis: 2
  }

  att_param {
    eos: 38
    batch_balance: 39
    loss_balance_delta: 1
  }
}

#####################################################################################
layer {
  name: "softmax"
  type: "Softmax"
  bottom: "RNNDec"
  top: "softmax"
  softmax_param {
    axis: 2
  }
}

layer {
  name: "permutesoftmax"
  type: "Permute"
  bottom: "softmax"
  top: "permutesoftmax"
  permute_param {
  order: 1
  order: 0
  }
}

layer {
  name: "ins0" 
  type: "InnerProduct" 
  bottom: "permutesoftmax"
  top: "ins0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 39
    weight_filler {
      type: "constant"
    }
    bias_filler {
      type: "constant"
    }
  }
  propagate_down: false
}

layer {
  name: "ins1"
  bottom: "ins0"
  top: "ins1"
  type: "Softmax"
  softmax_param {
    axis: 1
  }
}

layer {
  name: "W0" 
  type: "InnerProduct"
  bottom: "permutesoftmax"
  top: "W0"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "constant"
    }
    bias_filler {
      type: "fixed"
      values: 0
      values: 0
      values: 0
    }
  }
  propagate_down: false
}

layer {
  name: "W1"
  bottom: "W0"
  top: "W1"
  type: "Softmax"
  softmax_param {
    axis: 1
  }
}

layer {
  name: "EditProbabilityLoss"
  type: "EditProbabilityLoss"
  bottom: "softmax"
  bottom: "recog_label"
  bottom: "ins1"
  bottom: "W1"
  top: "EditProbabilityLoss"
  loss_weight: 1
  att_param {
    eos: 38
    str_map: "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ$_#"
  }
  edit_probability_param {
  }
  loss_param {
    normalize: true
  }
  propagate_down: true
  propagate_down: false
  propagate_down: true
  propagate_down: true
}

layer {
  name: "EditProbabilityAccuracy"
  type: "EditProbabilityAccuracy"
  bottom: "softmax"
  bottom: "recog_label"
  bottom: "ins1"
  bottom: "W1"
  top: "ACC"
  top: "ED"
  include { phase: TEST }
  edit_probability_param {
    no_log: true
  }
  att_param {
    eos: 38
    str_map: "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ$_#"
  }
}
